{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Direct Oral Anticoagulant (DOACs) prediction based on Patient's data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author: Muhammad Ayub Ansari__ <br>\n",
    "__Email: ayub_ansari@outlook.com__<br>\n",
    "__LinkedIN: https://www.linkedin.com/in/muhammad-ayub-ansari-74301065/__<br>\n",
    "\n",
    "<br>\n",
    "This project aims to predict the best Direct Oral Anticoagulants and the their right dose amount for patients with various different pre-exsisting conditions. The side effects of previously used DOACs and the obesity level of the patients were also considered in predicitng the most suitable and safe DOACs and their dose amount.\n",
    "<br>\n",
    "Currently in use DOACs and their doses are shown in the table below. \n",
    "<br>\n",
    "\n",
    "\n",
    "| DOAC | Doses |\n",
    "| :- | :- |\n",
    "| Apixaban | 2.5mg, 5mg,10mg \n",
    "| Rivaroxaban | 75mg,110mg,150mg \n",
    "| Edoxaban | 30mg,60mg \n",
    "| Dabigatran | 10mg,15mg,20mg \n",
    "\n",
    "<br>\n",
    "The dataset was accquired from NHS. The data is too noisy and consisted of too many error values and missing values. Moreover, the feature space is big and many features are not relevant or important for the task at hand. The data will be cleaned via vatious pre-processing steps. The final features will be used for the model traing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the version of Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\n",
      "pandas: 1.5.0\n",
      "numpy: 1.23.3\n",
      "seaborn: 0.12.0\n",
      "sklearn: 1.1.2\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "print('Python: {}'.format(sys.version))\n",
    "# pandas\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "# numpy\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "# seaborn\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "# scikit-learn\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data = DOAC2022.xlsx\n",
    "__Reading data from csv file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"DOAC2022.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRN</th>\n",
       "      <th>Ethnic Group</th>\n",
       "      <th>PersonSK</th>\n",
       "      <th>EncounterSK</th>\n",
       "      <th>Length Of Stay</th>\n",
       "      <th>EventSk</th>\n",
       "      <th>Age at Given Date</th>\n",
       "      <th>EventType</th>\n",
       "      <th>AC AddedTo EPR</th>\n",
       "      <th>AC Stated PerformedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>Emergency Admissions Upto 36 Months After PerformedTime</th>\n",
       "      <th>Mat Admissions Within 3 Years</th>\n",
       "      <th>Admit Method</th>\n",
       "      <th>Has Comorbidity Recorded on Spell</th>\n",
       "      <th>Has Bleeding Event (or Stroke) on Spell</th>\n",
       "      <th>Has Stroke on Spell</th>\n",
       "      <th>Has Thrombosis on Spell</th>\n",
       "      <th>Treatment_days</th>\n",
       "      <th>AC_END_DATE</th>\n",
       "      <th>AC_START_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>White - British</td>\n",
       "      <td>5859512</td>\n",
       "      <td>39337940</td>\n",
       "      <td>9</td>\n",
       "      <td>832948603</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>2018-10-03 17:31:09</td>\n",
       "      <td>2018-10-03 17:25:00</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Accident and Emergency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1117.0</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>3/10/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96</td>\n",
       "      <td>White - British</td>\n",
       "      <td>5859512</td>\n",
       "      <td>39337940</td>\n",
       "      <td>9</td>\n",
       "      <td>834980349</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>2018-10-04 16:36:44</td>\n",
       "      <td>2018-10-04 16:36:00</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Accident and Emergency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>4/10/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>White - British</td>\n",
       "      <td>5859512</td>\n",
       "      <td>39337940</td>\n",
       "      <td>9</td>\n",
       "      <td>837113509</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>2018-10-05 17:10:49</td>\n",
       "      <td>2018-10-05 17:10:00</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Accident and Emergency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>5/10/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>White - British</td>\n",
       "      <td>5859512</td>\n",
       "      <td>39337940</td>\n",
       "      <td>9</td>\n",
       "      <td>838714390</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>2018-10-06 16:58:47</td>\n",
       "      <td>2018-10-06 16:58:00</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Accident and Emergency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1114.0</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>6/10/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>White - British</td>\n",
       "      <td>5859512</td>\n",
       "      <td>39337940</td>\n",
       "      <td>9</td>\n",
       "      <td>840199866</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>2018-10-07 17:17:19</td>\n",
       "      <td>2018-10-07 17:17:00</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Accident and Emergency</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>7/10/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MRN     Ethnic Group  PersonSK  EncounterSK Length Of Stay    EventSk  \\\n",
       "0   96  White - British   5859512     39337940              9  832948603   \n",
       "1   96  White - British   5859512     39337940              9  834980349   \n",
       "2   96  White - British   5859512     39337940              9  837113509   \n",
       "3   96  White - British   5859512     39337940              9  838714390   \n",
       "4   96  White - British   5859512     39337940              9  840199866   \n",
       "\n",
       "   Age at Given Date    EventType      AC AddedTo EPR AC Stated PerformedTime  \\\n",
       "0                 71  Rivaroxaban 2018-10-03 17:31:09     2018-10-03 17:25:00   \n",
       "1                 71  Rivaroxaban 2018-10-04 16:36:44     2018-10-04 16:36:00   \n",
       "2                 71  Rivaroxaban 2018-10-05 17:10:49     2018-10-05 17:10:00   \n",
       "3                 71  Rivaroxaban 2018-10-06 16:58:47     2018-10-06 16:58:00   \n",
       "4                 71  Rivaroxaban 2018-10-07 17:17:19     2018-10-07 17:17:00   \n",
       "\n",
       "   ... Emergency Admissions Upto 36 Months After PerformedTime  \\\n",
       "0  ...                                                  3        \n",
       "1  ...                                                  3        \n",
       "2  ...                                                  3        \n",
       "3  ...                                                  3        \n",
       "4  ...                                                  3        \n",
       "\n",
       "  Mat Admissions Within 3 Years            Admit Method  \\\n",
       "0                             0  Accident and Emergency   \n",
       "1                             0  Accident and Emergency   \n",
       "2                             0  Accident and Emergency   \n",
       "3                             0  Accident and Emergency   \n",
       "4                             0  Accident and Emergency   \n",
       "\n",
       "   Has Comorbidity Recorded on Spell Has Bleeding Event (or Stroke) on Spell  \\\n",
       "0                                Yes                                      No   \n",
       "1                                Yes                                      No   \n",
       "2                                Yes                                      No   \n",
       "3                                Yes                                      No   \n",
       "4                                Yes                                      No   \n",
       "\n",
       "  Has Stroke on Spell Has Thrombosis on Spell Treatment_days AC_END_DATE  \\\n",
       "0                  No                      No         1117.0  2021-10-24   \n",
       "1                  No                      No         1116.0  2021-10-24   \n",
       "2                  No                      No         1115.0  2021-10-24   \n",
       "3                  No                      No         1114.0  2021-10-24   \n",
       "4                  No                      No         1113.0  2021-10-24   \n",
       "\n",
       "  AC_START_DATE  \n",
       "0     3/10/2018  \n",
       "1     4/10/2018  \n",
       "2     5/10/2018  \n",
       "3     6/10/2018  \n",
       "4     7/10/2018  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (183552, 50)\n",
      "Index(['MRN', 'Ethnic Group', 'PersonSK', 'EncounterSK', 'Length Of Stay',\n",
      "       'EventSk', 'Age at Given Date', 'EventType', 'AC AddedTo EPR',\n",
      "       'AC Stated PerformedTime', 'AC Stated Performed Date',\n",
      "       'AC Stated Performed Week', 'AC Stated Performed Month', 'AC Amount',\n",
      "       'AC untits', 'EGFR Type', 'EGFR Stated Date', 'EGFR Result 60 Or Less',\n",
      "       'EGFR Result', 'EGFR Units', 'Gender', 'Date Of Death',\n",
      "       'Deceased in EPR', 'Clinical Display', 'Bleeding Risk',\n",
      "       'Bleeding Risk Time', 'VTE Risk', 'VTE Risk Time', 'Height',\n",
      "       'Height Time', 'Weight', 'Weight Time', 'Calculated BMI', 'Actual BMI',\n",
      "       'Actual BMI Time', 'BMI Type', 'BMI Result', 'BMI Stated Date',\n",
      "       'BMI Flag', 'Difference in BMIs',\n",
      "       'Emergency Admissions Upto 36 Months After PerformedTime',\n",
      "       'Mat Admissions Within 3 Years', 'Admit Method',\n",
      "       'Has Comorbidity Recorded on Spell',\n",
      "       'Has Bleeding Event (or Stroke) on Spell', 'Has Stroke on Spell',\n",
      "       'Has Thrombosis on Spell', 'Treatment_days', 'AC_END_DATE',\n",
      "       'AC_START_DATE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# As the dataset has too many features, it cannot be fully displayed here. Let's print the columns in data.\n",
    "print(\"Shape of data: \", data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 Droping irrelevant columns__\n",
    "<br>\n",
    "Firstly, lets get rid of the all irrelevant features/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['MRN', 'PersonSK', 'EncounterSK','EventSk', 'AC AddedTo EPR','AC Stated PerformedTime', \n",
    "                  'AC Stated Performed Date','AC Stated Performed Week', 'AC Stated Performed Month', \n",
    "                  'AC untits', 'EGFR Type', 'EGFR Stated Date', 'EGFR Result 60 Or Less','EGFR Result', \n",
    "                  'EGFR Units',  'Date Of Death','Clinical Display', 'Bleeding Risk Time','VTE Risk Time',\n",
    "                  'Height Time', 'Weight Time','Actual BMI Time', 'BMI Stated Date','Difference in BMIs',\n",
    "                  'Mat Admissions Within 3 Years', 'Admit Method','AC_END_DATE','AC_START_DATE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (183552, 22)\n",
      "Index(['Ethnic Group', 'Length Of Stay', 'Age at Given Date', 'EventType',\n",
      "       'AC Amount', 'Gender', 'Deceased in EPR', 'Bleeding Risk', 'VTE Risk',\n",
      "       'Height', 'Weight', 'Calculated BMI', 'Actual BMI', 'BMI Type',\n",
      "       'BMI Result', 'BMI Flag',\n",
      "       'Emergency Admissions Upto 36 Months After PerformedTime',\n",
      "       'Has Comorbidity Recorded on Spell',\n",
      "       'Has Bleeding Event (or Stroke) on Spell', 'Has Stroke on Spell',\n",
      "       'Has Thrombosis on Spell', 'Treatment_days'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data: \", data.shape)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ethnic Group</th>\n",
       "      <th>Length Of Stay</th>\n",
       "      <th>Age at Given Date</th>\n",
       "      <th>EventType</th>\n",
       "      <th>AC Amount</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Deceased in EPR</th>\n",
       "      <th>Bleeding Risk</th>\n",
       "      <th>VTE Risk</th>\n",
       "      <th>Height</th>\n",
       "      <th>...</th>\n",
       "      <th>Actual BMI</th>\n",
       "      <th>BMI Type</th>\n",
       "      <th>BMI Result</th>\n",
       "      <th>BMI Flag</th>\n",
       "      <th>Emergency Admissions Upto 36 Months After PerformedTime</th>\n",
       "      <th>Has Comorbidity Recorded on Spell</th>\n",
       "      <th>Has Bleeding Event (or Stroke) on Spell</th>\n",
       "      <th>Has Stroke on Spell</th>\n",
       "      <th>Has Thrombosis on Spell</th>\n",
       "      <th>Treatment_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White - British</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>154.94</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BMI Score Measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White - British</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>BMI Score Measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White - British</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>BMI Score Measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>White - British</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>BMI Score Measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White - British</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>Rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>BMI Score Measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1113.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ethnic Group Length Of Stay  Age at Given Date    EventType  AC Amount  \\\n",
       "0  White - British              9                 71  Rivaroxaban       15.0   \n",
       "1  White - British              9                 71  Rivaroxaban       15.0   \n",
       "2  White - British              9                 71  Rivaroxaban       15.0   \n",
       "3  White - British              9                 71  Rivaroxaban       15.0   \n",
       "4  White - British              9                 71  Rivaroxaban       15.0   \n",
       "\n",
       "   Gender Deceased in EPR Bleeding Risk VTE Risk  Height  ... Actual BMI  \\\n",
       "0  Female              No          High     High  154.94  ...        NaN   \n",
       "1  Female              No          High     High     158  ...      20.63   \n",
       "2  Female              No          High     High     158  ...      20.63   \n",
       "3  Female              No          High     High     158  ...      20.63   \n",
       "4  Female              No          High     High     158  ...      20.63   \n",
       "\n",
       "             BMI Type       BMI Result BMI Flag  \\\n",
       "0  BMI Score Measured  18.5 - 20 kg/m2       No   \n",
       "1  BMI Score Measured  18.5 - 20 kg/m2       No   \n",
       "2  BMI Score Measured  18.5 - 20 kg/m2       No   \n",
       "3  BMI Score Measured  18.5 - 20 kg/m2       No   \n",
       "4  BMI Score Measured  18.5 - 20 kg/m2       No   \n",
       "\n",
       "  Emergency Admissions Upto 36 Months After PerformedTime  \\\n",
       "0                                                  3        \n",
       "1                                                  3        \n",
       "2                                                  3        \n",
       "3                                                  3        \n",
       "4                                                  3        \n",
       "\n",
       "  Has Comorbidity Recorded on Spell  Has Bleeding Event (or Stroke) on Spell  \\\n",
       "0                               Yes                                       No   \n",
       "1                               Yes                                       No   \n",
       "2                               Yes                                       No   \n",
       "3                               Yes                                       No   \n",
       "4                               Yes                                       No   \n",
       "\n",
       "  Has Stroke on Spell Has Thrombosis on Spell Treatment_days  \n",
       "0                  No                      No         1117.0  \n",
       "1                  No                      No         1116.0  \n",
       "2                  No                      No         1115.0  \n",
       "3                  No                      No         1114.0  \n",
       "4                  No                      No         1113.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of 50 features, we are left with 22 releavnt features. We will further drop more features as it required but for the time being we need all these features.\n",
    "<br>\n",
    "Let's give our feature meaningful names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rename Columns__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'Ethnic Group': 'Ethnicity', 'Length Of Stay':'LOS', 'Age at Given Date':'Age',\n",
    "        'Deceased in EPR':'Mortality', 'Bleeding Risk':'Bleeding_Risk', 'VTE Risk':'VTE_Risk',\n",
    "       'Emergency Admissions Upto 36 Months After PerformedTime':'Emergency_visits',\n",
    "       'Has Comorbidity Recorded on Spell':'Comorbidity','Has Bleeding Event (or Stroke) on Spell':'Bleeding', \n",
    "       'Has Stroke on Spell':'Stroke',\n",
    "       'Has Thrombosis on Spell':'Thrombosis'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Ethnicity', 'LOS', 'Age', 'EventType', 'AC Amount', 'Gender',\n",
      "       'Mortality', 'Bleeding_Risk', 'VTE_Risk', 'Height', 'Weight',\n",
      "       'Calculated BMI', 'Actual BMI', 'BMI Type', 'BMI Result', 'BMI Flag',\n",
      "       'Emergency_visits', 'Comorbidity', 'Bleeding', 'Stroke', 'Thrombosis',\n",
      "       'Treatment_days'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - Replacing \"-\" with N/A__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(\"-\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - Missing values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ethnicity               0\n",
       "LOS                   971\n",
       "Age                     0\n",
       "EventType               0\n",
       "AC Amount               4\n",
       "Gender                  0\n",
       "Mortality               0\n",
       "Bleeding_Risk        3919\n",
       "VTE_Risk             3867\n",
       "Height              54351\n",
       "Weight              47908\n",
       "Calculated BMI      62037\n",
       "Actual BMI          67282\n",
       "BMI Type            58537\n",
       "BMI Result          58537\n",
       "BMI Flag                0\n",
       "Emergency_visits        0\n",
       "Comorbidity             0\n",
       "Bleeding                0\n",
       "Stroke                  0\n",
       "Thrombosis              0\n",
       "Treatment_days      17544\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull( ).sum( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Lenght of stay, bleeding and VTE side effect feaatures have some of missing values. One of the most important feature is the BMI of the patient. The BMI value determines the obesisty level of the patient and the medicine and dose recommendation depends significantly on the obesity level. We will try to calculte the correct BMI value and hence the obesity level from height, weight, calculated bmi, actual bmi, bmi type and bmi flag features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethnicity           0\n",
      "LOS                 0\n",
      "Age                 0\n",
      "EventType           0\n",
      "AC Amount           0\n",
      "Gender              0\n",
      "Mortality           0\n",
      "Bleeding_Risk       0\n",
      "VTE_Risk            0\n",
      "Height              0\n",
      "Weight              0\n",
      "Calculated BMI      0\n",
      "Actual BMI          0\n",
      "BMI Type            0\n",
      "BMI Result          0\n",
      "BMI Flag            0\n",
      "Emergency_visits    0\n",
      "Comorbidity         0\n",
      "Bleeding            0\n",
      "Stroke              0\n",
      "Thrombosis          0\n",
      "Treatment_days      0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84380 entries, 1 to 183550\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Ethnicity         84380 non-null  object \n",
      " 1   LOS               84380 non-null  object \n",
      " 2   Age               84380 non-null  int64  \n",
      " 3   EventType         84380 non-null  object \n",
      " 4   AC Amount         84380 non-null  float64\n",
      " 5   Gender            84380 non-null  object \n",
      " 6   Mortality         84380 non-null  object \n",
      " 7   Bleeding_Risk     84380 non-null  object \n",
      " 8   VTE_Risk          84380 non-null  object \n",
      " 9   Height            84380 non-null  object \n",
      " 10  Weight            84380 non-null  object \n",
      " 11  Calculated BMI    84380 non-null  object \n",
      " 12  Actual BMI        84380 non-null  object \n",
      " 13  BMI Type          84380 non-null  object \n",
      " 14  BMI Result        84380 non-null  object \n",
      " 15  BMI Flag          84380 non-null  object \n",
      " 16  Emergency_visits  84380 non-null  int64  \n",
      " 17  Comorbidity       84380 non-null  object \n",
      " 18  Bleeding          84380 non-null  object \n",
      " 19  Stroke            84380 non-null  object \n",
      " 20  Thrombosis        84380 non-null  object \n",
      " 21  Treatment_days    84380 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(18)\n",
      "memory usage: 14.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Drop rows with missing values\n",
    "data = data.dropna()\n",
    "print(data.isnull( ).sum( ))\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - Case consistency__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __As most of the features are object, convert all of them to lower case strings for case consistency. Later we will convert the numeric features back to numeric data types from objects__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84380 entries, 1 to 183550\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Ethnicity         84380 non-null  object\n",
      " 1   LOS               84380 non-null  object\n",
      " 2   Age               84380 non-null  object\n",
      " 3   EventType         84380 non-null  object\n",
      " 4   AC Amount         84380 non-null  object\n",
      " 5   Gender            84380 non-null  object\n",
      " 6   Mortality         84380 non-null  object\n",
      " 7   Bleeding_Risk     84380 non-null  object\n",
      " 8   VTE_Risk          84380 non-null  object\n",
      " 9   Height            84380 non-null  object\n",
      " 10  Weight            84380 non-null  object\n",
      " 11  Calculated BMI    84380 non-null  object\n",
      " 12  Actual BMI        84380 non-null  object\n",
      " 13  BMI Type          84380 non-null  object\n",
      " 14  BMI Result        84380 non-null  object\n",
      " 15  BMI Flag          84380 non-null  object\n",
      " 16  Emergency_visits  84380 non-null  object\n",
      " 17  Comorbidity       84380 non-null  object\n",
      " 18  Bleeding          84380 non-null  object\n",
      " 19  Stroke            84380 non-null  object\n",
      " 20  Thrombosis        84380 non-null  object\n",
      " 21  Treatment_days    84380 non-null  object\n",
      "dtypes: object(22)\n",
      "memory usage: 14.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data = data.apply(lambda x: x.astype(str).str.lower())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>LOS</th>\n",
       "      <th>Age</th>\n",
       "      <th>EventType</th>\n",
       "      <th>AC Amount</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Mortality</th>\n",
       "      <th>Bleeding_Risk</th>\n",
       "      <th>VTE_Risk</th>\n",
       "      <th>Height</th>\n",
       "      <th>...</th>\n",
       "      <th>Actual BMI</th>\n",
       "      <th>BMI Type</th>\n",
       "      <th>BMI Result</th>\n",
       "      <th>BMI Flag</th>\n",
       "      <th>Emergency_visits</th>\n",
       "      <th>Comorbidity</th>\n",
       "      <th>Bleeding</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Thrombosis</th>\n",
       "      <th>Treatment_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white - british</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>bmi score measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white - british</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>bmi score measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white - british</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>bmi score measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>white - british</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>bmi score measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>white - british</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>rivaroxaban</td>\n",
       "      <td>15.0</td>\n",
       "      <td>female</td>\n",
       "      <td>no</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>20.63</td>\n",
       "      <td>bmi score measured</td>\n",
       "      <td>18.5 - 20 kg/m2</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1112.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Ethnicity LOS Age    EventType AC Amount  Gender Mortality  \\\n",
       "1  white - british   9  71  rivaroxaban      15.0  female        no   \n",
       "2  white - british   9  71  rivaroxaban      15.0  female        no   \n",
       "3  white - british   9  71  rivaroxaban      15.0  female        no   \n",
       "4  white - british   9  71  rivaroxaban      15.0  female        no   \n",
       "5  white - british   9  71  rivaroxaban      15.0  female        no   \n",
       "\n",
       "  Bleeding_Risk VTE_Risk Height  ... Actual BMI            BMI Type  \\\n",
       "1          high     high    158  ...      20.63  bmi score measured   \n",
       "2          high     high    158  ...      20.63  bmi score measured   \n",
       "3          high     high    158  ...      20.63  bmi score measured   \n",
       "4          high     high    158  ...      20.63  bmi score measured   \n",
       "5          high     high    158  ...      20.63  bmi score measured   \n",
       "\n",
       "        BMI Result BMI Flag Emergency_visits Comorbidity Bleeding Stroke  \\\n",
       "1  18.5 - 20 kg/m2       no                3         yes       no     no   \n",
       "2  18.5 - 20 kg/m2       no                3         yes       no     no   \n",
       "3  18.5 - 20 kg/m2       no                3         yes       no     no   \n",
       "4  18.5 - 20 kg/m2       no                3         yes       no     no   \n",
       "5  18.5 - 20 kg/m2       no                3         yes       no     no   \n",
       "\n",
       "  Thrombosis Treatment_days  \n",
       "1         no         1116.0  \n",
       "2         no         1115.0  \n",
       "3         no         1114.0  \n",
       "4         no         1113.0  \n",
       "5         no         1112.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__Let's covert numeric features back to int or float data types.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84380 entries, 1 to 183550\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Ethnicity         84380 non-null  int64  \n",
      " 1   LOS               84380 non-null  int64  \n",
      " 2   Age               84380 non-null  int64  \n",
      " 3   EventType         84380 non-null  object \n",
      " 4   AC Amount         84380 non-null  float64\n",
      " 5   Gender            84380 non-null  object \n",
      " 6   Mortality         84380 non-null  object \n",
      " 7   Bleeding_Risk     84380 non-null  object \n",
      " 8   VTE_Risk          84380 non-null  object \n",
      " 9   Height            84380 non-null  object \n",
      " 10  Weight            84380 non-null  object \n",
      " 11  Calculated BMI    84380 non-null  object \n",
      " 12  Actual BMI        84380 non-null  object \n",
      " 13  BMI Type          84380 non-null  object \n",
      " 14  BMI Result        84380 non-null  object \n",
      " 15  BMI Flag          84380 non-null  object \n",
      " 16  Emergency_visits  84380 non-null  int64  \n",
      " 17  Comorbidity       84380 non-null  object \n",
      " 18  Bleeding          84380 non-null  object \n",
      " 19  Stroke            84380 non-null  object \n",
      " 20  Thrombosis        84380 non-null  object \n",
      " 21  Treatment_days    84380 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(16)\n",
      "memory usage: 14.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data[\"Age\"] = pd.to_numeric(data[\"Age\"])\n",
    "data[\"Emergency_visits\"] = pd.to_numeric(data[\"Emergency_visits\"])\n",
    "data[\"AC Amount\"] = pd.to_numeric(data[\"AC Amount\"])\n",
    "data[\"Treatment_days\"] = pd.to_numeric(data[\"Treatment_days\"])\n",
    "data[\"LOS\"] = pd.to_numeric(data[\"LOS\"])\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__Now our data is free of missing values and consistent in terms of case.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - Lable encoding__\n",
    "<br>\n",
    "The categorical features such as thnicity, gender etc will be encoded to appropriate numeric labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['white - british', 'black or black british - caribbean',\n",
       "       'white - any other white background', 'other - not stated',\n",
       "       'white - irish', 'other - any other ethnic group',\n",
       "       'asian or asian british - pakistani',\n",
       "       'asian - any other asian background',\n",
       "       'mixed - white and black caribbean',\n",
       "       'black or black british - african',\n",
       "       'asian or asian british - indian',\n",
       "       'black - any other black background', 'other - chinese',\n",
       "       'other - not known', 'mixed - white and asian',\n",
       "       'asian or asian british - bangladeshi',\n",
       "       'mixed - any other mixed background'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Ethnicity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of wokring with 17 different ethinicities. Lets group them in three broader groups.\n",
    "#Group1: White (Consisting of whites from UK or any part of the world)\n",
    "#Group2: BAME (Consisting of Black and Asian)\n",
    "#Group3: Others (All remaining)\n",
    "data['Ethnicity'] = data['Ethnicity'].replace(['white - british', \n",
    "              'white - any other white background', \n",
    "              'white - irish'], 1)\n",
    "data['Ethnicity'] = data['Ethnicity'].replace(['black or black british - caribbean', \n",
    "              'asian or asian british - pakistani', \n",
    "              'asian - any other asian background',\n",
    "              'mixed - white and black caribbean',\n",
    "              'black or black british - african',\n",
    "              'asian or asian british - indian',\n",
    "              'black - any other black background',\n",
    "              'mixed - white and asian',\n",
    "              'asian or asian british - bangladeshi',\n",
    "              'mixed - any other mixed background'], 2)\n",
    "data['Ethnicity'] = data['Ethnicity'].replace(['other - not stated', \n",
    "              'other - any other ethnic group', \n",
    "              'other - chinese',\n",
    "             'other - not known'], 3)\n",
    "\n",
    "## Convert the data type from Object to numeric\n",
    "data[\"Ethnicity\"] = pd.to_numeric(data[\"Ethnicity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84380 entries, 1 to 183550\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Ethnicity         84380 non-null  int64  \n",
      " 1   LOS               84380 non-null  int64  \n",
      " 2   Age               84380 non-null  int64  \n",
      " 3   EventType         84380 non-null  object \n",
      " 4   AC Amount         84380 non-null  float64\n",
      " 5   Gender            84380 non-null  object \n",
      " 6   Mortality         84380 non-null  object \n",
      " 7   Bleeding_Risk     84380 non-null  object \n",
      " 8   VTE_Risk          84380 non-null  object \n",
      " 9   Height            84380 non-null  object \n",
      " 10  Weight            84380 non-null  object \n",
      " 11  Calculated BMI    84380 non-null  object \n",
      " 12  Actual BMI        84380 non-null  object \n",
      " 13  BMI Type          84380 non-null  object \n",
      " 14  BMI Result        84380 non-null  object \n",
      " 15  BMI Flag          84380 non-null  object \n",
      " 16  Emergency_visits  84380 non-null  int64  \n",
      " 17  Comorbidity       84380 non-null  object \n",
      " 18  Bleeding          84380 non-null  object \n",
      " 19  Stroke            84380 non-null  object \n",
      " 20  Thrombosis        84380 non-null  object \n",
      " 21  Treatment_days    84380 non-null  float64\n",
      "dtypes: float64(2), int64(4), object(16)\n",
      "memory usage: 14.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Ethnicity'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The ethinicity features has only three classes now and converted to numeric feature.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EventType__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rivaroxaban', 'apixaban', 'dabigatran etexilate', 'dabigatran',\n",
       "       'edoxaban'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['EventType'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EventType feature represent the medicine. There are four different types of DOACs. Lets merge two version of Dabigatran into one. So, that we will have four unique DOACs in the dataset.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rivaroxaban', 'apixaban', 'dabigatran', 'edoxaban'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['EventType'] = data['EventType'].replace(['dabigatran etexilate', 'dabigatran'], 'dabigatran')\n",
    "data['EventType'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__AC Amount__\n",
    "<br>Since our goal is to pridict the DOAC and its correct dose, it is better to merge thw two columns into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rivaroxaban15.0' 'apixaban5.0' 'dabigatran110.0' 'rivaroxaban20.0'\n",
      " 'apixaban2.5' 'apixaban10.0' 'dabigatran150.0' 'edoxaban60.0'\n",
      " 'edoxaban30.0' 'rivaroxaban10.0' 'apixaban1.0' 'dabigatran75.0'\n",
      " 'rivaroxaban2.5']\n"
     ]
    }
   ],
   "source": [
    "#Lets combine EventType(medicine) and AC amount(dose) columns.\n",
    "data[\"AC_Amount\"] = data[\"EventType\"] + data[\"AC Amount\"].astype(str)\n",
    "print(data[\"AC_Amount\"].unique())\n",
    "\n",
    "## Lets drop Eventtype and AC Amount columns. As they no longer required.\n",
    "data = data.drop(['EventType', 'AC Amount'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84380 entries, 1 to 183550\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Ethnicity         84380 non-null  int64  \n",
      " 1   LOS               84380 non-null  int64  \n",
      " 2   Age               84380 non-null  int64  \n",
      " 3   Gender            84380 non-null  object \n",
      " 4   Mortality         84380 non-null  object \n",
      " 5   Bleeding_Risk     84380 non-null  object \n",
      " 6   VTE_Risk          84380 non-null  object \n",
      " 7   Height            84380 non-null  object \n",
      " 8   Weight            84380 non-null  object \n",
      " 9   Calculated BMI    84380 non-null  object \n",
      " 10  Actual BMI        84380 non-null  object \n",
      " 11  BMI Type          84380 non-null  object \n",
      " 12  BMI Result        84380 non-null  object \n",
      " 13  BMI Flag          84380 non-null  object \n",
      " 14  Emergency_visits  84380 non-null  int64  \n",
      " 15  Comorbidity       84380 non-null  object \n",
      " 16  Bleeding          84380 non-null  object \n",
      " 17  Stroke            84380 non-null  object \n",
      " 18  Thrombosis        84380 non-null  object \n",
      " 19  Treatment_days    84380 non-null  float64\n",
      " 20  AC_Amount         84380 non-null  object \n",
      "dtypes: float64(1), int64(4), object(16)\n",
      "memory usage: 16.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gender__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in gender:  ['female' 'male']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 84380 entries, 1 to 183550\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Ethnicity         84380 non-null  int64  \n",
      " 1   LOS               84380 non-null  int64  \n",
      " 2   Age               84380 non-null  int64  \n",
      " 3   Gender            84380 non-null  int64  \n",
      " 4   Mortality         84380 non-null  object \n",
      " 5   Bleeding_Risk     84380 non-null  object \n",
      " 6   VTE_Risk          84380 non-null  object \n",
      " 7   Height            84380 non-null  object \n",
      " 8   Weight            84380 non-null  object \n",
      " 9   Calculated BMI    84380 non-null  object \n",
      " 10  Actual BMI        84380 non-null  object \n",
      " 11  BMI Type          84380 non-null  object \n",
      " 12  BMI Result        84380 non-null  object \n",
      " 13  BMI Flag          84380 non-null  object \n",
      " 14  Emergency_visits  84380 non-null  int64  \n",
      " 15  Comorbidity       84380 non-null  object \n",
      " 16  Bleeding          84380 non-null  object \n",
      " 17  Stroke            84380 non-null  object \n",
      " 18  Thrombosis        84380 non-null  object \n",
      " 19  Treatment_days    84380 non-null  float64\n",
      " 20  AC_Amount         84380 non-null  object \n",
      "dtypes: float64(1), int64(5), object(15)\n",
      "memory usage: 16.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in gender: \", data['Gender'].unique())\n",
    "data['Gender'] = data['Gender'].replace(['male','female'], [1,2])\n",
    "\n",
    "## Convert the data type from Object to numeric\n",
    "data[\"Gender\"] = pd.to_numeric(data[\"Gender\"])\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing \"Warfarin\" drug aka event type from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.EventType != \"warfarin\"]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replce EGFR Results >90 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"EGFR Result\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[\"EGFR Result\"].replace({\">90\": 100}, inplace=True)\n",
    "data1[\"EGFR Result\"].replace({\"Not applicable\": 0}, inplace=True)\n",
    "data1[\"EGFR Result\"].replace({\"Probably contaminated with Potassium EDTA, suggest rpt.\": 0}, inplace=True)\n",
    "data1[\"EGFR Result\"].replace({\"No sample received\": 0}, inplace=True)\n",
    "data1[\"EGFR Result\"].replace({\"Comment\": 0}, inplace=True)\n",
    "data1[\"EGFR Result\"].replace({\"Not Available\": 0}, inplace=True)\n",
    "data1[\"EGFR Result\"].replace({\"** INSUFFICIENT SPECIMEN **\": 0}, inplace=True)\n",
    "data1[\"EGFR Result\"].replace({\"Grossly haemolysed\": 0}, inplace=True)\n",
    "data1[\"EGFR Result\"] = pd.to_numeric(data1[\"EGFR Result\"])\n",
    "print(data1[\"EGFR Result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dabigatran and dabigatran etexilate from eventtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.loc[data1['EventType']==\"dabigatran etexilate\", 'EventType'] = \"dabigatran\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.EventType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv (\"clean_DOAC2022.csv\", index = False, header=True, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean DOAC2022 CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean_DOAC2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of data: \", df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull( ).sum( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Stay\n",
    "replace missing values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Length Of Stay\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Length Of Stay\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into 4 sub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df = df[df['BMI Flag'] == 'yes']\n",
    "not_obese_df = df[df['BMI Flag'] == 'no']\n",
    "normal_kidney_df = df[df['EGFR Result 60 Or Less'] == 'no']\n",
    "kidney_df = df[df['EGFR Result 60 Or Less'] == 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df.to_csv (\"obese_DOAC2022.csv\", index = False, header=True, encoding='utf-8')\n",
    "not_obese_df.to_csv (\"non_obese_DOAC2022.csv\", index = False, header=True, encoding='utf-8')\n",
    "normal_kidney_df.to_csv (\"normal_kidney_DOAC2022.csv\", index = False, header=True, encoding='utf-8')\n",
    "kidney_df.to_csv (\"kidney_DOAC2022.csv\", index = False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Obese Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df = pd.read_csv(\"obese_DOAC2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of data: \", obese_df.shape)\n",
    "print(obese_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df.isnull( ).sum( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values in EGFR Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obese_df[\"EGFR Result\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df[\"EGFR Result\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since all the missing values in EGFR results hvae no in the corresponding \"EGFR Result 60 Or Less\" attribute. \n",
    "#### So, its better to replace the missing values with \n",
    "#### the average of only those records whoes corresponding EGFR values are greater than 60 or \"no\" in \"EGFR result 60 or less\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = obese_df[[\"EGFR Result\", \"EGFR Result 60 Or Less\"]]\n",
    "print(temp_df.head())\n",
    "\n",
    "tt = temp_df[temp_df['EGFR Result 60 Or Less'] == 'no']\n",
    "print(tt.head())\n",
    "print(tt.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing missing values in \"EGFR Result\" with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EGFR_mean = 82.13\n",
    "print(EGFR_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df[\"EGFR Result\"].fillna(EGFR_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values in Actual BMI\n",
    "\n",
    "Replacing missing values in Actual BMI from calculated BMI column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df['Actual BMI'].fillna(obese_df['Calculated BMI'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values in Bleeding risk and VTE risk\n",
    "Drop the values where 'VTE Risk'and 'Bleeding Risk' are both null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obese_df = obese_df[obese_df['Bleeding Risk'].notna() && obese_df['VTE Risk'].notna()]\n",
    "col_lst = ['Bleeding Risk', 'VTE Risk']\n",
    "obese_df.dropna(axis = 0, subset = col_lst, how = 'all', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values in AC Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df = obese_df[obese_df['AC Amount'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values in Bleeding risk, Height, weight and calculated BMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df = obese_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df.isnull( ).sum( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df.nunique( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethinic Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = obese_df[\"Ethnic Group\"].value_counts()\n",
    "print(kk)\n",
    "plt.figure();\n",
    "\n",
    "kk.plot(kind=\"pie\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age at Given Date - bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = obese_df['Age at Given Date'].value_counts()\n",
    "#kk = pd.value_counts(obese_df['Age at Given Date'])\n",
    "print(kk)\n",
    "plt.figure(figsize=(18, 8), dpi=80);\n",
    "plt.xlabel('Age at a given date')\n",
    "plt.ylabel('count')\n",
    "kk.plot(kind=\"bar\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EventType - pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = obese_df[\"EventType\"].value_counts()\n",
    "print(kk)\n",
    "plt.figure();\n",
    "\n",
    "kk.plot(kind=\"pie\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AC Amount -pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = obese_df[\"AC Amount\"].value_counts()\n",
    "print(kk)\n",
    "plt.figure(figsize=(10, 5), dpi=80);\n",
    "plt.xlabel('AC Amount')\n",
    "plt.ylabel('count')\n",
    "kk.plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleeding Risk -pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = obese_df[\"Bleeding Risk\"].value_counts()\n",
    "print(kk)\n",
    "plt.figure();\n",
    "\n",
    "kk.plot(kind=\"pie\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VTE Risk  - pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = obese_df[\"VTE Risk\"].value_counts()\n",
    "print(kk)\n",
    "plt.figure();\n",
    "\n",
    "kk.plot(kind=\"pie\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emergency Admissions Upto 36 Months After PerformedTime - bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = obese_df[\"Emergency Admissions Upto 36 Months After PerformedTime\"].value_counts()\n",
    "print(kk)\n",
    "plt.figure(figsize=(10, 5), dpi=80);\n",
    "plt.xlabel('Emergency Admissions Upto 36 Months After PerformedTime')\n",
    "plt.ylabel('count')\n",
    "kk.plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obese_df.to_csv (\"obese.csv\", index = False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert nominal values to numerical\n",
    "Material Attribute is converted as\n",
    "abs = 0, pla = 1\n",
    "Infill Pattrens are converted into\n",
    "grid = 0, honeycomb = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.material = [0 if each == \"abs\" else 1 for each in data.material]\n",
    "data.infill_pattern = [0 if each == \"grid\" else 1 for each in data.infill_pattern]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert numerical values to nominal ---> Numerical Binning\n",
    "data.elongation = ['small' if each >=0 and each <1  else 'Big' if  each >=1 and each <3   else 'very_Big' for each in data.elongation]\n",
    "\n",
    "'small' = [0,1], 'big' =[1,3], 'very big' = [3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['elongation'] = pd.cut(data['elongation'], bins=[0,1,3,4], labels=[\"small\", \"big\", \"very_big\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of each class in class-attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['elongation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(data, hue='EventType', markers='+')\n",
    "g._legend.get_title().set_fontsize(20)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.violinplot(y='elongation', x='layer_height', data=data, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='elongation', x='wall_thickness', data=data, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='elongation', x='infill_density', data=data, inner='quartile')\n",
    "plt.show()\n",
    "g = sns.violinplot(y='elongation', x='infill_pattern', data=data, inner='quartile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "plt.figure(figsize=(12, 10)) ### Setting the size of the plot\n",
    "sns.heatmap(data = corr_matrix,cmap='BrBG', annot=True, linewidths=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "Normalization refers to rescaling real valued numeric attributes into the range 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X = preprocessing.normalize(X)\n",
    "print(normalized_X[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization\n",
    "Standardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_X = preprocessing.scale(X)\n",
    "print(standardized_X[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for any null/missing values in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need of Feature imputation as there is no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=standardized_X, orient=\"v\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]\n",
    "#Normalised Data\n",
    "normalized_X = preprocessing.normalize(X)\n",
    "# Standardised Data\n",
    "standardized_X = preprocessing.scale(X)\n",
    "#encode categorical data into digits\n",
    "y = pd.get_dummies(y)\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## Using Normal Data\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=1)\n",
    "# decision tree construction \n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', random_state=1)\n",
    "result_DT= dt.fit(X_train, y_train)\n",
    "#pridiction\n",
    "y_pred = dt.predict(X_test)\n",
    "#accuracy\n",
    "data_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Data Accuracy:\",data_accuracy)\n",
    "########################################## Using Normalised Data\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_X, y,test_size=0.3, random_state=1)\n",
    "# decision tree construction \n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', random_state=1)\n",
    "result_DT= dt.fit(X_train, y_train)\n",
    "#pridiction\n",
    "y_pred = dt.predict(X_test)\n",
    "#accuracy\n",
    "data_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Normalised data Accuracy:\",data_accuracy)\n",
    "########################################## Using Standardised Data\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardized_X, y,test_size=0.3, random_state=1)\n",
    "# decision tree construction \n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', random_state=1)\n",
    "result_DT= dt.fit(X_train, y_train)\n",
    "#pridiction\n",
    "y_pred = dt.predict(X_test)\n",
    "#accuracy\n",
    "data_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Standardised Accuracy:\",data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "namez =  ['Small','Big','very_Big']\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=namez, digits=2,output_dict=False)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "print('Confusion Matrix')\n",
    "species = np.array(y_test).argmax(axis=1)\n",
    "predictions = np.array(y_pred).argmax(axis=1)\n",
    "print(confusion_matrix(species, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance in classifier\n",
    "print(\"Variable importacne in the classifier.\")\n",
    "pd.concat((pd.DataFrame(data.iloc[:, 1:].columns, columns = ['variable']), \n",
    "           pd.DataFrame(result_DT.feature_importances_, columns = ['importance'])), \n",
    "          axis = 1).sort_values(by='importance', ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Creating Trees and printing on console. \n",
    "from sklearn import tree\n",
    "import graphviz \n",
    "##### setting environment variable of Path for Graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/ProgramData/Anaconda3/Library/bin/graphviz/'\n",
    "\n",
    "input_features = ['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']\n",
    "output_features = 'elongation'\n",
    "dot_data = tree.export_graphviz(result_DT, out_file=None, \n",
    "                      feature_names=input_features,  \n",
    "                      class_names=output_features,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph \n",
    "######### Creating Tree and storing as PDF\n",
    "#graph.render(\"DecisionTree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###                Hyper parameter tuning and cross validation              ###\n",
    "###############################################################################\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "##### 1 Setup the parameters\n",
    "param_dist = {\"max_depth\":[1,9],\n",
    "              \"min_samples_leaf\": [2,9],\n",
    "              \"criterion\": [\"gini\",\"entropy\"]}\n",
    "\n",
    "#### 2 instantiate a decision tree\n",
    "Dtree = DecisionTreeClassifier()\n",
    "\n",
    "#### 3 instantiate ranommizedsearchCV object\n",
    "tree_cv =  GridSearchCV(Dtree, param_dist, n_jobs=None, cv=10, verbose=0)\n",
    "\n",
    "#### 4 fit to the data\n",
    "Result = tree_cv.fit(X_train, y_train)\n",
    "DT_result = Result\n",
    "#### 5 print the tunned parameters and score\n",
    "print(\"Best parameters are: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {} \".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrap Aggregation(Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method.\n",
    "\n",
    "An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.\n",
    "\n",
    "Bootstrap Aggregation is a general procedure that can be used to reduce the variance for those algorithm that have high variance. An algorithm that has high variance are decision trees, like classification and regression trees (CART).\n",
    "\n",
    "Decision trees are sensitive to the specific data on which they are trained. If the training data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision tree can be quite different and in turn the predictions can be quite different.\n",
    "\n",
    "Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.\n",
    "\n",
    "Let’s assume we have a sample dataset of 1000 instances (x) and we are using the CART algorithm. Bagging of the CART algorithm would work as follows.\n",
    "\n",
    "1. Create many (e.g. 100) random sub-samples of our dataset with replacement.\n",
    "2. Train a CART model on each sample.\n",
    "3. Given a new dataset, calculate the average prediction from each model.\n",
    "For example, if we had 5 bagged decision trees that made the following class predictions for a in input sample: blue, blue, red, blue and red, we would take the most frequent class and predict blue.\n",
    "\n",
    "\n",
    "Source: https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "\"\"\"\n",
    "# example of grid searching key hyperparameters for BaggingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]\n",
    "\n",
    "# define models and parameters\n",
    "model = BaggingClassifier(random_state=1)\n",
    "n_estimators = [10, 100,200, 300, 1000]\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "BA_result = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are an improvement over bagged decision trees.\n",
    "\n",
    "A problem with decision trees like CART is that they are greedy. They choose which variable to split on using a greedy algorithm that minimizes error. As such, even with Bagging, the decision trees can have a lot of structural similarities and in turn have high correlation in their predictions.\n",
    "\n",
    "Combining predictions from multiple models in ensembles works better if the predictions from the sub-models are uncorrelated or at best weakly correlated.\n",
    "\n",
    "Random forest changes the algorithm for the way that the sub-trees are learned so that the resulting predictions from all of the subtrees have less correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#encode categorical data into digits\n",
    "y = pd.get_dummies(y)\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=1)\n",
    "# Random Forests construction \n",
    "rf = RandomForestClassifier(criterion='entropy', \n",
    "                             n_estimators=1000,\n",
    "                             min_samples_split=2,\n",
    "                             min_samples_leaf=1,\n",
    "                             max_features='auto',\n",
    "                             oob_score=True,\n",
    "                             random_state=1,\n",
    "                             n_jobs=-1,\n",
    "                             verbose= 0)\n",
    "result_RF = rf.fit(X_train, y_train)\n",
    "\n",
    "#pridiction\n",
    "y_pred = rf.predict(X_test)\n",
    "#accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "namez =  ['Small','Big','very_Big']\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=namez, digits=2,output_dict=False)) \n",
    "\n",
    "#confusion matrix\n",
    "species = np.array(y_test).argmax(axis=1)\n",
    "predictions = np.array(y_pred).argmax(axis=1)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(species, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Importance in Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance in classifier\n",
    "print(\"Variable importacne in the classifier.\")\n",
    "print(pd.concat((pd.DataFrame(data.iloc[:, 1:].columns, columns = ['variable']), \n",
    "           pd.DataFrame(result_RF.feature_importances_, columns = ['importance'])), \n",
    "          axis = 1).sort_values(by='importance', ascending = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###                Hyper parameter tuning and cross validation              ###\n",
    "###############################################################################\n",
    "\n",
    "y = pd.get_dummies(y)\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=1)\n",
    "\n",
    "##### 1 Setup the parameters\n",
    "param_dist = {\"max_depth\":[None,9],\n",
    "              \"min_samples_leaf\": [1,50],\n",
    "              \"max_features\":['sqrt', 'log2'],\n",
    "              \"n_estimators\":[10,1000],\n",
    "              \"criterion\": [\"gini\",\"entropy\"]\n",
    "              }\n",
    "\n",
    "#### 2 instantiate a decision tree\n",
    "RF_tree = RandomForestClassifier()\n",
    "\n",
    "#### 3 instantiate ranommizedsearchCV object\n",
    "tree_cv =  GridSearchCV(RF_tree, param_dist, n_jobs=-1, cv=5)\n",
    "\n",
    "#### 4 fit to the data\n",
    "Result = tree_cv.fit(X_train, y_train)\n",
    "\n",
    "#### 5 print the tunned parameters and score\n",
    "print(\"Best parameters are: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {} \".format(tree_cv.best_score_))\n",
    "RF_result = Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Nearest Neighbhours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors, or KNN for short, is one of the simplest machine learning algorithms and is used in a wide array of institutions. KNN is a non-parametric, lazy learning algorithm. When we say a technique is non-parametric, it means that it does not make any assumptions about the underlying data. In other words, it makes its selection based off of the proximity to other data points regardless of what feature the numerical values represent. Being a lazy learning algorithm implies that there is little to no training phase. Therefore, we can immediately classify new data points as they present themselves.\n",
    "\n",
    "Pros:\n",
    "No assumptions about data\n",
    "Simple algorithm — easy to understand\n",
    "Can be used for classification and regression\n",
    "Cons:\n",
    "High memory requirement — All of the training data must be present in memory in order to calculate the closest K neighbors\n",
    "Sensitive to irrelevant features\n",
    "Sensitive to the scale of the data since we’re computing the distance to the closest K points\n",
    "\n",
    "Source: https://towardsdatascience.com/k-nearest-neighbor-python-2fccc47d2a55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "\"\"\"\n",
    "\n",
    "# example of grid searching key hyperparametres for KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]\n",
    "# define models and parameters\n",
    "model = KNeighborsClassifier()\n",
    "n_neighbors = range(1, 21, 2)\n",
    "weights = ['uniform', 'distance']\n",
    "metric = ['euclidean', 'manhattan', 'minkowski']\n",
    "# define grid search\n",
    "grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0, verbose=0)\n",
    "######################################## Using raw data\n",
    "grid_result = grid_search.fit(X, y)\n",
    "KNN_result = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "########################################## Using Standardised Data\n",
    "print(\"Using Standardised Data\")\n",
    "grid_result = grid_search.fit(standardized_X, y)\n",
    "KNN_result_standard = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    " #   print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "########################################## Using Normalised Data\n",
    "print(\"Using Normalised Data\")\n",
    "grid_result = grid_search.fit(normalized_X, y)\n",
    "KNN_result_normal = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    " #   print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if 'y'is the predicted value.\n",
    "\n",
    "'y'(w, x) = w0 + w1.x1 + w2.x2 + ... + wp.xp\n",
    "\n",
    "Across the module, we designate the vector w1.x1 + w2.x2 + ... + wp.xp as coef_ and \n",
    "w0 as intercept_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we talk about Regression, we often end up discussing Linear and Logistic Regression. But, that’s not the end. Do you know there are 7 types of Regressions?\n",
    "\n",
    "Linear and logistic regression is just the most loved members from the family of regressions.  Last week, I saw a recorded talk at NYC Data Science Academy from Owen Zhang, Chief Product Officer at DataRobot. He said, ‘if you are using regression without regularization, you have to be very special!’. I hope you get what a person of his stature referred to.\n",
    "\n",
    "I understood it very well and decided to explore regularization techniques in detail.\n",
    "\n",
    "In this article, I have explained the complex science behind ‘Ridge Regression‘ and ‘Lasso Regression‘ which are the most fundamental regularization techniques used in data science, sadly still not used by many.\n",
    "Source: https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"data.csv\", sep = \",\")\n",
    "\n",
    "data.material = [0 if each == \"abs\" else 1 for each in data.material]\n",
    "data.infill_pattern = [0 if each == \"grid\" else 1 for each in data.infill_pattern]\n",
    "\n",
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]\n",
    "X.head()\n",
    "#y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnr_reg = LinearRegression()\n",
    "MSEs = cross_val_score(lnr_reg, X,y, scoring='neg_mean_squared_error', cv=50 )\n",
    "print(MSEs)\n",
    "print(\"Mean Squared Error\", np.mean(MSEs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ridged Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge()\n",
    "params = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-2,1e-1, 1,0.1,0.01,0.001,2,4,5]}\n",
    "grid_result = GridSearchCV(cv=50, scoring='neg_mean_squared_error', estimator=ridge_reg, param_grid=params)\n",
    "grid_result = grid_result.fit(X, y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "Ridge_result = grid_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "data.material = [0 if each == \"abs\" else 1 for each in data.material]\n",
    "data.infill_pattern = [0 if each == \"grid\" else 1 for each in data.infill_pattern]\n",
    "\n",
    "\n",
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]\n",
    "normalized_X = preprocessing.normalize(X)\n",
    "standardized_X = preprocessing.scale(X)\n",
    "lasso_reg = Lasso()\n",
    "params = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-2,1e-1, 1,0.1,0.01,0.001,2,4,5]}\n",
    "grid_resul = GridSearchCV(cv=50, scoring='neg_mean_squared_error', estimator=lasso_reg, param_grid=params)\n",
    "########################################## Using raw data\n",
    "grid_result =grid_result.fit(X, y)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "LASSO_result = grid_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data['elongation'] = pd.cut(data['elongation'], bins=[0,1,3,4], labels=[\"small\", \"big\", \"very_big\"])\n",
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]\n",
    "\n",
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0, verbose=0)\n",
    "\n",
    "\n",
    "grid_result = grid_search.fit(X, y)\n",
    "LR_result = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "########################################## Using Standardised Data\n",
    "print(\"Using Standardised Data\")\n",
    "grid_result = grid_search.fit(standardized_X, y)\n",
    "LR_result_standard = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "########################################## Using Normalised Data\n",
    "print(\"Using Normalised Data\")\n",
    "grid_result = grid_search.fit(normalized_X, y)\n",
    "LR_result_normal = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gradient Bossting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "data = pd.read_csv(\"data.csv\", sep = \",\")\n",
    "data.material = [0 if each == \"abs\" else 1 for each in data.material]\n",
    "data.infill_pattern = [0 if each == \"grid\" else 1 for each in data.infill_pattern]\n",
    "data['elongation'] = pd.cut(data['elongation'], bins=[0,1,3,4], labels=[\"small\", \"big\", \"very_big\"])\n",
    "\n",
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]\n",
    "\n",
    "# define models and parameters\n",
    "model = GradientBoostingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "\n",
    "\n",
    "grid_result = grid_search.fit(X, y)\n",
    "GBC_result = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "data = pd.read_csv(\"data.csv\", sep = \",\")\n",
    "data.material = [0 if each == \"abs\" else 1 for each in data.material]\n",
    "data.infill_pattern = [0 if each == \"grid\" else 1 for each in data.infill_pattern]\n",
    "data['elongation'] = pd.cut(data['elongation'], bins=[0,1,3,4], labels=[\"small\", \"big\", \"very_big\"])\n",
    "\n",
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "y = data[['elongation']]\n",
    "\n",
    "# define model and parameters\n",
    "model = SVC()\n",
    "kernel = ['poly', 'rbf', 'sigmoid']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "gamma = ['scale']\n",
    "# define grid search\n",
    "grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "cv = RepeatedStratifiedKFold(n_splits=20, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0, verbose=1)\n",
    "\n",
    "grid_result = grid_search.fit(X, y)\n",
    "SVM_result = grid_result\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "########################################## Using Standardised Data\n",
    "print(\"Using Standardised Data\")\n",
    "grid_result = grid_search.fit(standardized_X, y)\n",
    "SVM_result_standard = grid_result\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "########################################## Using Normalised Data\n",
    "print(\"Using Normalised Data\")\n",
    "grid_result = grid_search.fit(normalized_X, y)\n",
    "SVM_result_normal = grid_result\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "# Reading data from csv file\n",
    "data = pd.read_csv(\"data.csv\", sep = \",\")\n",
    "# Looking for any null/missing values in the data set\n",
    "print(\"Looking for any null/missing values in the data set\")\n",
    "print(data.isnull().any())\n",
    "#convert nominal values to numerical\n",
    "data.material = [0 if each == \"abs\" else 1 for each in data.material]                  # abs = 0, pla = 1\n",
    "data.infill_pattern = [0 if each == \"grid\" else 1 for each in data.infill_pattern]     # grid = 0, honeycomb = 1\n",
    "#convert numerical values to nominal ---> Numerical Binning\n",
    "data['elongation'] = pd.cut(data['elongation'], bins=[0,1,3,4], labels=[\"small\", \"big\", \"very_big\"])\n",
    "\n",
    "X = data[['layer_height', 'wall_thickness', 'infill_density', 'infill_pattern','nozzle_temperature', 'bed_temperature', 'print_speed', 'material','fan_speed']]\n",
    "Y = data[['elongation']]\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "\n",
    "'''    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_DOAC_NHS",
   "language": "python",
   "name": "ve_doac_nhs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
